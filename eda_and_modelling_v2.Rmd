---
title: "EDA_and_modelling_5oct"
author: "Michael Wang"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pre-processing Before EDA

Import Libraries

```{r}
library(tidyverse)
```

wd:

```{r}
setwd('/Users/michaelwang/Desktop/unsw/unsw\ actl/4305/Assignment')
getwd()
```

Read in the data

```{r}
df <- read_csv('/Users/michaelwang/Desktop/unsw/unsw actl/4305/Assignment/actl4305-ggins-datathon/standardised_freely_quote_data_v2.csv')

crime_indices <- read_csv('/Users/michaelwang/Desktop/unsw/unsw actl/4305/Assignment/cleaned_crime_indices.csv')
```

trip main df columns a bit

```{r}
df_trim <- df %>% 
  select(-destinations, -traveller_ages, -quote_create_time, -matches("^boost_.*_date$"))
```

Deal with remaining NA's in the data

```{r}
df_clean <- df_trim %>%
  mutate(
    across(starts_with("boost_"), ~ if_else(is.na(.x), 0, .x)),
    quote_hour = if_else(quote_hour > 23, 24, quote_hour)
    )

any(is.na(df_clean))
```

Glimpse tells us we should turn discount into a number, and then turn the remaining character classes into factors

```{r}
# glimpse(df_clean)
```

```{r}
df_discount <- df_clean %>%
  mutate(
    discount = as.numeric(str_extract(discount, "^[0-9]*"))
  )

# gets a names vector, then applies the logical mask to extract only the columns.
character_colnames <- names(df_discount)[sapply(df_discount, is.character)]

# all_of interprets a character vector as column names
df_factors <- df_discount %>%
  mutate(
    across(all_of(character_colnames), ~ as.factor(.x))
  )
```

```{r}
# glimpse(df_factors)
```

Attach crime scores

```{r}
df_crime_scores <- df_factors %>%
  mutate(quote_id = row_number()) %>%
  left_join(crime_indices, by = "quote_id") %>%
  select(-quote_id)

df_crime_scores <- df_crime_scores %>%
  rowwise() %>%
  mutate(
    median_crime_index = median(as.numeric(unlist(strsplit(crime_index, ", "))))
  ) %>%
  ungroup %>%
  select(-crime_index)
```

```{r}
plot(df_crime_scores$quote_hour)
```

```{r}
df_clean_quote_hour <- df_crime_scores %>%
  mutate(quote_hour = if_else(quote_hour > 23, 24, quote_hour))
```

```{r}
table(df_clean_quote_hour$extra_cancellation)
```

```{r}
df_extra_cancellation <- df_clean_quote_hour %>%
  mutate(
    extra_cancel_tier = case_when(
      extra_cancellation == 0 ~ 0,
      extra_cancellation <= 5000 ~ 1,
      extra_cancellation > 5000 ~ 2,
      TRUE ~ 1
    )
  ) %>%
    select(-extra_cancellation)

df_final <- df_extra_cancellation %>%
  select(-quote_time)

```

Backup before eda

```{r}
df_eda <- df_final
```

```{r}
# df_write <- df_eda
# write_csv(df_write, '/Users/michaelwang/Desktop/unsw/unsw actl/4305/Assignment/final_eda_modelling_data.csv')
```

==================================

# EDA

```{r}
# library(tidyverse)
# df_eda <- read_csv('/Users/michaelwang/Desktop/unsw/unsw actl/4305/Assignment/final_eda_modelling_data.csv')
```

```{r}
# install.packages("DataExplorer")
# install.packages("skimr")
library(DataExplorer)
library(skimr)
```

```{r}
# plot_intro(df_eda)
```

```{r}
# skim(df_eda)
```

Variables to process outliers first:

-   quote price, lead length, trip length,

Variables to transform first before checking for outliers:

```{r}
# skim_df <- skim(df_eda)
```

Overall data frame:

1.  median start date: 2024 12 27
2.  median end date: 2025 01 18
3.  mean quote price 361, median 201 —\> right skewed with some large outliers (super long trips probably)
4.  mean quote hour is 14.6, median is 14 very similar. Approx 2pm is best for some reason
5.  mean trip length is 27 days, median is 15 days. Confirms number 3
6.  mean lead length is 60 days, median 25
7.  mean and median 1.8/2 travelers
8.  mean/median max age 50/51
9.  mean/median min age 42/39 so high...?
10. mean age range 8, median 0
11. mean of mean and median age 26, median is 42/44
12. mean/median 1.22/1 generation
13. mean/median 0.5/0 boosts
14. 0.026 proportion to africa, 0.188 to europe, 0.221 to oceania, 0.028 to multi region, 0.065 domestic cruise, 0.008 international cruise, 0.121 north america, 0.006 central america, 0.017 south america, 0.029 middle east, 0.003 central asia, 0.296 south east asia, 0.034 south asia, 0.147 east asia, 0.002 antarctica (doesn't sum to one since people go to multiple places. Sums to approx 1.191)
15. 0.1 have kids under 12, 0.069 has teens over 13, 0.75 has adults O 18, 0.3 have seniors O 60, 0.15 is family.
16. top quote times are around 12pm, 4pm, and 1pm
17. imbalanced data —\> 60999 NO, 9001 YES
18. 0.065 have snowsports, 0.009 adventure activities, 0.105 cruise cover, 0.001 medical, 0.033 gadget, 0.025 motorcycle, 0.016 rental vehicle excess, 0.008 specified items.
19. avg 1.6 adults
20. mean of median crime index of all visited destinations is 42.44, median 46.1 - slightly right skewed
21. not sure if boost lengths will tell anything because they represent different things.
    1.  boost 1: mean length 9 days
    2.  boost 2: 1.619 days
    3.  boost 3: 0.366 days
    4.  boost 4: 0.134 days
    5.  boost 5: 0.056 days
    6.  boost 6: 0.027 days
    7.  boost 7: 0.015 days
    8.  boost 8: 0.013 days
22. For each boost length, max value is VERY LARGE. 547, 545, 463, 365, 347, 192, 192, 192

```{r}
# plot_bar(df_eda)
```

1.  large amount of quotes coming from O60 couples, single U60, single O60, single U30, couple U60, and then parents plus kids. Remaining probably is \< 10000 total quotes.
2.  most is 15 or 0 discount, makes sense cause thats the auto discount for couple or single as above.

```{r}
# plot_histogram(df_eda)
```

```{r}
# plot_density(df_eda)
```

Mean age bimodal at 25 and 60 ish. Num travellers spikes at 1 and 2. Quote hour max at around 14-15. Boost num spikes at 0 and 1. Crime index spikes at approx 48.

Split convert and no convert and visualise

```{r}
# df_eda_convert <- df_eda %>%
#   filter(convert == "YES")
# 
# df_eda_nonconvert <- df_eda %>%
#   filter(convert == "NO")
```

```{r}
# skim(df_eda_convert)
```

```{r}
# skim_df_convert <- skim(df_eda_convert)
```

1.  has child 0.117, has teen 0.082, has adult 0.878, has senior 0.1667, is family 0.179
2.  quote price 193 mean 132 median
3.  extra cancellation 271 mean 0 median
4.  quote hour 14.7 mean 15 median.
5.  trip length 20 days mean 13 days median
6.  lead length 33.6 mean 10 median
7.  num travellers 1.8/2 mean/median
8.  44.8/43 max age mean/median, 35.7/30 min mean/median
9.  age range mean 9, median 0
10. mean/median age, mean is 40 median is 34.
11. 1.2/1 generations mean/median
12. 0.66/0 boosts mean/median
13. 0.019 Africa, 0.155 europe, 0.188 oceania, 0.020 multi region, 0.072 domestic cruise, 0.010 international cruise, 0.089 north america, 0.003 central america, 0.013 south america, 0.018 middle east, 0.001 central asia, 0.355 south east asia, 0.026 south asia, 0.202 east asia, 0.001 antarctica
14. 0.152 snowsports, 0.020 adventure, 0.100 cruise cover, 0.001 medical, 0.084 gadget, 0.069 motorcycle, 0.023 rental vehicle excess, 0.010 specified items
15. 1.6 mean adults, 1 median
16. mean median crime index over all locations is 41, median is 46. Max median crime score is 81.
17. Boost lengths: 5.8, 2.7, 0.7, 0.27, 0.11, 0.03, 0.016, 0.015 mean. Max: 539, 545, 364, 364, 347, 30, 38, 38.

```{r}
# skim(df_eda_nonconvert)
```

```{r}
# skim_df_nonconvert <- skim(df_eda_nonconvert)
```

1.  0.1 has child u12, 0.067 has teen o13, 0.73 have adults 018, 0.32 have seniors o65, 0.15 are families
2.  386 mean quote price, 215 median quote price
3.  1755 mean extra cancellation, 0 median
4.  quote hour mean 14.5, median 14
5.  28 day mean trip length, 16 median
6.  1.83/2 mean/median travelers
7.  51/53 max age mean/median
8.  43/41 min age mean/median
9.  8/0 age range mean/median
10. 47/45, 47/46 mean/median of mean age and median age
11. 1.2/1 mean/median num generations
12. 0.55/0 mean/median boost number
13. 0.027 africa, 0.194 europe, 0.226 oceania, 0.029 multi region, 0.064 domestic cruise, 0.008 int cruise, 0.126 north america, 0.006 central america, 0.018 south america, 0.024 middle east, 0.001 central asia, 0.288 south east asia, 0.035 south asia, 0.139 east asia, 0.002 antarctica
14. 0.052 snow sports, 0.007 adventure, 0.107 cruise cover, 0.0007 medical, 0.025 gadget cover, 0.018 motorcycle, 0.015 rental vehicle excess, 0.008 specific items
15. 1.6/2 mean/median num adults
16. boost lengths:
    1.  9.67, 1.46, 0.312, 0.114, 0.048, 0.027, 0.015, 0.013,
17. crime score mean 42.65, median 46.1

```{r}
# plot_bar(df_eda_convert)
```

For converted quotes:

-   mostly single U60 and single U30, then couple U60, couple O60, then single O60, parnets plus kids, couples U30, parents plus teens. everything else is less than 500 out of 9001

```{r}
# plot_bar(df_eda_nonconvert)
```

For non-converted:

-   mostly couples O60, single O60, single U60, single U30, couple U60, parents plus kids, couple U30. Everything else under 4000 out of 60999

```{r}
# plot_histogram(df_eda_convert)
```

For converted quotes:

-   mean age is much more centered around 25 as opposed to bimodal in overall data. boost number is larger in general

-   Everything else very similar.

```{r}
# plot_histogram(df_eda_nonconvert)
```

For non-convert:

-   ages are split better between the two peaks 25 and 60, less boosts overall, longer lead lengths, generally older population.

-   everything else very similar.

```{r}
# plot_density(df_eda_convert)
```

```{r}
# plot_density(df_eda_nonconvert)
```

EDA DONE:

-   need to deal with outliers for a few variables to remove possible errors. Winsorize and cap at 99th percentile

    -   quote price, trip length, lead length, boost lengths.

-   box cox for positive non-normal continuous columns, then z score scale them:

    -   all above + age vars +
    -   Will test without box cox as well

-   add binary flag if 0's dominate. Otherwise z score scale –\> discrete, count based variables with low ranges

# WINSORISATION

only 200 quotes over 5000, and even less super large ones. 173 trips over a year long. Winsorize/cap at 99th percentile.

```{r}
df_large_quote_price <- df_eda[df_eda$quote_price > 5000,]
df_large_lengths <- df_eda[df_eda$trip_length > 365,]
```

```{r}
df_capped <- df_eda
```

```{r}
cap_trip_len <- quantile(df_capped$trip_length, 0.99)
df_capped$trip_length_capped <- pmin(df_capped$trip_length, cap_trip_len)

cap_price <- quantile(df_capped$quote_price, 0.99)
df_capped$quote_price_capped <- pmin(df_capped$quote_price, cap_price)

cap_lead_len <- quantile(df_capped$lead_length, 0.99)
df_capped$lead_length_capped <- pmin(df_capped$lead_length, cap_lead_len)

boost_cols_1_2 <- paste0("boost_", 1:2, "_length")
for (col in boost_cols_1_2) {
  cap_val <- quantile(df_capped[[col]], 0.99)
  df_capped[[col]] <- pmin(df_capped[[col]], cap_val)
}

boost_cols_3_4_5 <- paste0("boost_", 3:5, "_length")
for (col in boost_cols_3_4_5) {
  cap_val <- quantile(df_capped[[col]], 0.999)
  df_capped[[col]] <- pmin(df_capped[[col]], cap_val)
}

boost_cols_6_7_8 <- paste0("boost_", 6:8, "_length")
for (col in boost_cols_6_7_8) {
  cap_val <- quantile(df_capped[[col]], 0.999)
  df_capped[[col]] <- pmin(df_capped[[col]], cap_val)
}

cap_age_range <- quantile(df_capped$age_range, 0.99)
df_capped$age_range_capped <- pmin(df_capped$age_range, cap_age_range)

df_capped <- df_capped %>%
  select(-trip_length, -quote_price, -lead_length, -age_range)
```

# Correlation Analysis

```{r}
df_correlation <- df_capped %>%
  select(-median_age, -min_age, -max_age)
# 
# cor(df_correlation[ , sapply(df_correlation, is.numeric)])
```

Main ones over 0.2 or less than -0.2:

discount and num travellers 0.65

boost num and boost 1, 2, 3 (2 \> 3 \> 1) 0.55, 0.5, 0.45

boost num and snow sports 0.33

domestic cruise and oceania 0.49

oceania and south east asia -0.33

boost num and cruise cover 0.41

snow sports and east asia 0.28

boost num and gadget 0.35

crime index and east asia -0.69, and oceania 0.29, africa 0.23

extra cancel teir with discount 0.49, with discount 0.2

trip length capped and europe 0.21

quote price capped and mean age?? 0.33 and north america is 0.32,

lead length capped and mean age 0.35, and europe is 0.21

age range capped and discount 0.41, num travellers 0.8, mean age -0.3, generations 0.79

mean age and domestic cruise 0.2

north america and south east asia -0.22

```{r}
df_correlation <- df_correlation %>%
  select(-generations, -age_range_capped, -boost_4_length, -boost_5_length, -boost_6_length, -boost_7_length, -boost_8_length)
```

```{r}
df_transformations <- df_correlation
```

# Box-Cox + z Score Transformations

```{r}
library(caret)
```

Box cox all the age related ones, the length related ones and the price.

Can comment out box cox to test modelling without box cox

```{r}
box_cox_cols <- c(
  "mean_age",
  "trip_length_capped",
  "quote_price_capped",
  "lead_length_capped",
  paste0("boost_", 1:3, "_length")
)

small <- 1e-6
for (col in box_cox_cols) {
  zero_flag <- min(df_transformations[[col]])
  if (zero_flag <= 0) {
    df_transformations[[col]] <- df_transformations[[col]] + small
  }
}

X <- as.data.frame(df_transformations[, box_cox_cols])

preprocess_boxcox <- preProcess(X, method = "BoxCox")
X_boxcox  <- predict(preprocess_boxcox, X)

df_transformations[paste0(names(X_boxcox), "_boxcox")] <- X_boxcox
```

```{r}
# uncomment this
# z_cols <- c(
#   "mean_age",
#   "trip_length_capped",
#   "quote_price_capped",
#   "lead_length_capped",
#   paste0("boost_", 1:3, "_length")
# )

z_cols <- paste0(box_cox_cols, "_boxcox")
X <- as.data.frame(df_transformations[, z_cols])
preprocess_z <- preProcess(X, method = c("center", "scale"))
X_z <- predict(preprocess_z, X)

# and this if testing without boxcox
# df_transformations[paste0(z_cols, "_z")] <- X_z

df_transformations[paste0(box_cox_cols, "_boxcox_z")] <- X_z

# and this
# df_transformations <- df_transformations %>%
#   select(-all_of(z_cols))

df_transformations <- df_transformations %>%
  select(-ends_with("_boxcox"), -all_of(box_cox_cols))
```

```{r}
df_transformations_1 <- df_transformations
```

z score transform everything else: the discrete columns (discount, boost num, num travellers and adults, generations) + crime index

```{r}
remaining_z_cols <- c(
  "median_crime_index",
  "num_adults",
  "boost_num",
  "num_travellers",
  "discount"
)

small <- 1e-6
for (col in remaining_z_cols) {
  zero_flag <- min(df_transformations_1[[col]])
  if (zero_flag <= 0) {
    df_transformations_1[[col]] <- df_transformations_1[[col]] + small
  }
}

X_z_df <- as.data.frame(df_transformations_1[, remaining_z_cols])
preprocess_z <- preProcess(X_z_df, method = c("center", "scale"))
X_z <- predict(preprocess_z, X_z_df)
df_transformations_1[paste0(remaining_z_cols, "_z")] <- X_z

df_transformations_1 <- df_transformations_1 %>%
  select(-all_of(remaining_z_cols))
```

```{r}
# summary(df_transformations_1)
```

```{r}
# glimpse(df_transformations_1)
```

# Modelling

```{r}
# install.packages("xgboost")
library(glmnet)
library(randomForest)
# library(xgboost)
```

Transformed and scaled data for logistic reg + shrinkage

```{r}
df_modelling <- df_transformations_1
```

```{r}
# write_csv(df_modelling, '/Users/michaelwang/Desktop/unsw/unsw actl/4305/Assignment/pre_modelling_transformed_data.csv')
```

# No imb data proc

### Train/Test/Val

```{r}
# model.matrix converts to numeric matrix to run regression, [, -1] removes convert column
X <- model.matrix(convert ~ ., df_modelling)[, -1]
y <- ifelse(df_modelling$convert == "YES", 1, 0)

set.seed(111)
# regular sampling:
# train_index <- sample(seq_len(nrow(X)), 0.8 * nrow(X))

# stratified sampling to make sure test and train have equal proportions of converted and non-converted customers - checking proportions it seems like the original sampling was already pretty good. Still, worth making precise.
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
# 0.25 of 0.8 is 0.2 of total

X_train <- X[train_index,]
y_train <- y[train_index]

validation_index <- createDataPartition(y_train, p = 0.25, list = FALSE)
X_val <- X_train[validation_index,]
y_val <- y_train[validation_index]
X_train_inner <- X_train[-validation_index, ]
y_train_inner <- y_train[-validation_index]

X_test <- X[-train_index,]
y_test <- y[-train_index]

# length(y_train_inner) / length(y)
# length(y_val) / length(y)
# length(y_test) / length(y)

validation_index_df <- createDataPartition(Train_Data$convert, p = 0.25, list = FALSE)

Train_Data <- df_modelling[train_index,]
Validation_Data <- Train_Data[validation_index_df, ]
Train_Data_inner<- Train_Data[-validation_index_df, ]
Test_Data <- df_modelling[-train_index,]
y_test_glm <- ifelse(Test_Data$convert == "YES", 1, 0)
y_validation_glm <- ifelse(Validation_Data$convert == "YES", 1, 0)

```

### GLM/Shrinkage

```{r}
glm_basic <- glm(convert ~ ., data = Train_Data, family = binomial)
# summary(glm_basic)
```

```{r}
# ridge (alpha = 0), lasso (alpha = 1), elastic Net (0 < alpha < 1)
# then check best one. At the same time, also runs default 10 fold cv on training data to pick best lambda
# hyperparameter for shrinkage to control degree of penalty applied.
cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1)
cv_ridge <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 0)
cv_enet <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 0.5)
```

```{r}
# # cv_lasso, etc. are cv.glmnet objects - running coef on them returns a sparse coeff matrix with one col of the
# # fitted coeffecients, with intercept in first row. This is with the best lambda, stored in the cv.glmnet object.
coef_lasso <- coef(cv_lasso, s = "lambda.min")
# coef_ridge <- coef(cv_ridge, s = "lambda.min")
# coef_enet <- coef(cv_enet, s = "lambda.min")

# # this extracts that column of coeffecients, keeps only the non 0 ones, and keeps it as a matrix (drop = false to
# prevent dropping into a vector); then returns top 20.
lasso_nonzero <- coef_lasso[coef_lasso[,1] != 0, , drop = FALSE]
sort(lasso_nonzero[,1], decreasing = TRUE)[1:20]

# ridge_nonzero <- coef_ridge[coef_ridge[,1] != 0, , drop = FALSE]
# sort(ridge_nonzero[,1], decreasing = TRUE)[1:20]

# enet_nonzero <- coef_enet[coef_enet[,1] != 0, , drop = FALSE]
# sort(enet_nonzero[,1], decreasing = TRUE)[1:20]

# # cv error vs log(lambda)
# plot(cv_lasso)
# plot(cv_ridge)
# plot(cv_enet)
```

```{r}
# type = response means to give probabilities for logistic reg instead of values for normal regression
# predict() as a universal apply data to model base r function

# idea of using dataframe for glm prediction and matrix for shrinkage prediction i think is because of the code inside
# the glm model saw the original data, so it was the test data in the same form to help process. The shrinkage never
# saw the dataframe only a matrix rep, so thus it wants it in the same format.

pred_glm <- predict(glm_basic, newdata = Test_Data, type = "response")
pred_lasso <- predict(cv_lasso, newx = X_test, type = "response", s = "lambda.min")
pred_ridge <- predict(cv_ridge, newx = X_test, type = "response", s = "lambda.min")
pred_enet <- predict(cv_enet, newx = X_test, type = "response", s = "lambda.min")

glm_class <- ifelse(pred_glm > 0.5, 1, 0)
lasso_class <- ifelse(pred_lasso > 0.5, 1, 0)
ridge_class <- ifelse(pred_ridge > 0.5, 1, 0)
enet_class <- ifelse(pred_enet > 0.5, 1, 0)

acc_glm <- mean(glm_class == y_test_glm)
acc_lasso <- mean(lasso_class == y_test)
acc_ridge <- mean(ridge_class == y_test)
acc_enet <- mean(enet_class == y_test)

c(glm = acc_glm, Lasso = acc_lasso, Ridge = acc_ridge, ElasticNet = acc_enet)
```

```{r}
calculate_metrics <- function(predictions, actual) {
  tp <- sum(predictions == 1 & actual == 1)
  tn <- sum(predictions == 0 & actual == 0)
  fp <- sum(predictions == 1 & actual == 0)
  fn <- sum(predictions == 0 & actual == 1)
  
  accuracy <- (tp + tn) / (tp + tn + fp + fn)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  sensitivity <- recall
  specificity <- tn / (tn + fp)
  f1 <- 2 * precision * recall / (precision + recall)
  
  return(c(
    Accuracy = accuracy,
    Precision = precision,
    Recall = recall,
    Specificity = specificity,
    F1_Score = f1,
  ))
}

metrics_glm <- calculate_metrics(glm_class, y_test_glm)
metrics_lasso <- calculate_metrics(lasso_class, y_test)
metrics_ridge <- calculate_metrics(ridge_class, y_test)
metrics_enet  <- calculate_metrics(enet_class, y_test)

comparison_table <- data.frame(
  GLM = metrics_glm,
  Lasso = metrics_lasso,
  Ridge = metrics_ridge,
  ElasticNet = metrics_enet
)

print(round(comparison_table, 4))
```

```{r}
library(pROC)
roc_glm <- roc(y_test_glm, as.vector(pred_glm))
roc_lasso <- roc(y_test, as.vector(pred_lasso))
roc_ridge <- roc(y_test, as.vector(pred_ridge))
roc_enet <- roc(y_test, as.vector(pred_enet))

auc(roc_glm)
auc(roc_lasso)
auc(roc_ridge)
auc(roc_enet)
```

LASSO for more interpretability (feature selection)

ENET for slightly more stable coeffecients

HORRENDOUS F1 SCORE when not treating any imbalanced data stuff.

## RF

```{r}
df_tree_modelling <- df_eda %>%
  select(-median_age, -min_age, -max_age, -generations, -age_range)
```

```{r}
set.seed(111)
df_tree_modelling$convert <- factor(df_tree_modelling$convert, , levels = c("NO","YES"))

y_tree <- df_tree_modelling$convert
train_index <- createDataPartition(y_tree, p = 0.8, list = FALSE)

y_train_tree <- y[train_index]
validation_index <- createDataPartition(y_train_tree, p = 0.25, list = FALSE)

tree_train <- df_tree_modelling[train_index, ]
tree_train_inner <- df_tree_modelling[-validation_index, ]
tree_val <- tree_train[validation_index, ]
tree_test <- df_tree_modelling[-train_index, ]
y_val_tree <- as.numeric(ifelse(tree_val$convert == "YES", 1, 0))
y_test_tree <- as.numeric(ifelse(tree_test$convert == "YES", 1, 0))

set.seed(222)
rf <- randomForest(
  convert ~ .,
  data = tree_train,
  ntree = 500,
  mtry = floor(sqrt(ncol(tree_train) - 1)),
  importance = TRUE
)

# type = prob returns probabilities instead of class labels. This is for several reasons,
# first is calculating metrics like ROC or just probability in general, and second for
# later decision threshold tuning. [, "YES"] means we select the YES probability column.
pred_test <- predict(rf, tree_test, type = "prob")[, "YES"]
pred_class <- ifelse(pred_test >= 0.5, 1, 0)
rf_metrics <- calculate_metrics(pred_class, y_test_tree)

# RUN THESE ONLY
rf_metrics
print(rf)
varImpPlot(rf)
```

# Imb data procs

## GLM DTO

```{r}
optimise_threshold <- function(predictions, actual, metric = "f1") {
  thresholds <- seq(0.1, 0.9, by = 0.01)
  results <- data.frame(
    threshold = thresholds,
    accuracy = NA,
    precision = NA,
    recall = NA,
    f1 = NA,
    specificity = NA
  )
  
  # quick logic for this loop: predictions is a numeric vector of probabilities. Use an if else to change this vector
  # into class labels 1 or 0 according to the new threshold. Scan across all the thresholds, calculate true pos, true
  # neg, false pos and false neg. Then use these to calculate the diff metrics to eventually get to F1 score.
  
  # At the end, we have a results data frame. Append each metric into the result data frame, such that each metric in 
  # the result data frame is a list of the metrics for each threshold level.
  
  for (i in seq_along(thresholds)) {
    threshold <- thresholds[i]
    pred_class <- ifelse(predictions > threshold, 1, 0)
    
    tp <- sum(pred_class == 1 & actual == 1)
    tn <- sum(pred_class == 0 & actual == 0)
    fp <- sum(pred_class == 1 & actual == 0)
    fn <- sum(pred_class == 0 & actual == 1)
    
    results$accuracy[i] <- (tp + tn) / (tp + tn + fp + fn)
    results$precision[i] <- tp / (tp + fp)
    results$recall[i] <- tp / (tp + fn)
    results$f1[i] <- 2 * results$precision[i] * results$recall[i] / (results$precision[i] + results$recall[i])
    results$specificity[i] <- tn / (tn + fp)
  }
  
  # which picks out the index of the list which is the highest. We can decide which metric to use. Here we default
  # to F1 since its the best indicator overall esp for this imbalanced data.
  # then we extract the threshold that corresponds do it, and return everything.
  
  optimal_index <- which.max(results[[metric]])
  optimal_threshold <- results$threshold[optimal_index]
  
  return(list(
    optimal_threshold = optimal_threshold,
    all_results = results,
    best_metrics = results[optimal_index, ]
  ))
}
```

```{r}
glm_dt_opt <- glm(convert ~ ., data = Train_Data_inner, family = binomial)
cv_lasso_dt_opt <- cv.glmnet(X_train_inner, y_train_inner,
                             family = "binomial", alpha = 1)
cv_ridge_dt_opt <- cv.glmnet(X_train_inner, y_train_inner,
                             family = "binomial", alpha = 0)
cv_enet_dt_opt <- cv.glmnet(X_train_inner, y_train_inner,
                            family = "binomial", alpha = 0.5)

pred_glm_dt_opt_val <- predict(glm_dt_opt, newdata = Validation_Data, type = "response")
pred_lasso_dt_opt_val <- predict(cv_lasso_dt_opt, newx = X_val, type = "response",
                      s = "lambda.min")
pred_ridge_dt_opt_val <- predict(cv_ridge_dt_opt, newx = X_val, type = "response",
                      s = "lambda.min")
pred_enet_dt_opt_val <- predict(cv_enet_dt_opt,  newx = X_val, type = "response",
                     s = "lambda.min")

opt_glm_dt_opt <- optimise_threshold(pred_glm_dt_opt_val, y_validation_glm, metric = "f1")
opt_lasso_dt_opt <- optimise_threshold(pred_lasso_dt_opt_val, y_val, metric = "f1")
opt_ridge_dt_opt <- optimise_threshold(pred_ridge_dt_opt_val, y_val, metric = "f1")
opt_enet_dt_opt <- optimise_threshold(pred_enet_dt_opt_val, y_val, metric = "f1")
```

```{r}
res_lasso_dt_opt <- opt_lasso_dt_opt$all_results

ggplot(res_lasso_dt_opt, aes(x = threshold, y = f1)) +
  geom_line() +
  geom_vline(xintercept = opt_lasso_dt_opt$optimal_threshold, linetype = 2) +
  geom_point(data = subset(res_lasso_dt_opt,
                           threshold == opt_lasso_dt_opt$optimal_threshold),
             aes(x = threshold, y = f1), size = 2) +
  labs(title = "Decision Threshold Optimisation (F1)",
       subtitle = paste("Optimal threshold =",
                        round(opt_lasso_dt_opt$optimal_threshold, 3)),
       x = "Threshold", y = "F1 score") +
  theme_minimal()
```

```{r}
# ValidatioN Results After Tuning Decision Threshold
res_glm_dt_opt <- opt_glm_dt_opt$all_results
res_ridge_dt_opt <- opt_ridge_dt_opt$all_results
res_enet_dt_opt <- opt_enet_dt_opt$all_results

res_glm_dt_opt[res_glm_dt_opt$threshold == opt_glm_dt_opt$optimal_threshold,]
res_lasso_dt_opt[res_lasso_dt_opt$threshold == opt_lasso_dt_opt$optimal_threshold,]
res_ridge_dt_opt[res_ridge_dt_opt$threshold == opt_ridge_dt_opt$optimal_threshold,]
res_enet_dt_opt[res_enet_dt_opt$threshold == opt_enet_dt_opt$optimal_threshold,]
```

```{r}
# Model Eval Using Test Set
pred_glm_dt_opt <- predict(glm_dt_opt, newdata = Test_Data, type = "response")
pred_lasso_dt_opt <- predict(cv_lasso_dt_opt, newx = X_test, type = "response",
                      s = "lambda.min")
pred_ridge_dt_opt <- predict(cv_ridge_dt_opt, newx = X_test, type = "response",
                      s = "lambda.min")
pred_enet_dt_opt <- predict(cv_enet_dt_opt,  newx = X_test, type = "response",
                     s = "lambda.min")

glm_class_dt_opt <- ifelse(pred_glm_dt_opt > opt_glm_dt_opt$optimal_threshold, 1, 0)
lasso_class_dt_opt <- ifelse(pred_lasso_dt_opt > opt_lasso_dt_opt$optimal_threshold,
                             1, 0)
ridge_class_dt_opt <- ifelse(pred_ridge_dt_opt > opt_ridge_dt_opt$optimal_threshold,
                             1, 0)
enet_class_dt_opt <- ifelse(pred_enet_dt_opt  > opt_enet_dt_opt$optimal_threshold, 1, 0)

metrics_glm_dt_opt <- calculate_metrics(glm_class_dt_opt, y_test_glm)
metrics_lasso_dt_opt <- calculate_metrics(lasso_class_dt_opt, y_test)
metrics_ridge_dt_opt <- calculate_metrics(ridge_class_dt_opt, y_test)
metrics_enet_dt_opt <- calculate_metrics(enet_class_dt_opt,  y_test)

comparison_table <- data.frame(
  GLM = metrics_glm_dt_opt,
  Lasso = metrics_lasso_dt_opt,
  Ridge = metrics_ridge_dt_opt,
  ElasticNet = metrics_enet_dt_opt
)
comparison_table

```

## RF DTO

```{r}
set.seed(222)
rf_dto <- randomForest(
  convert ~ .,
  data = tree_train_inner,
  ntree = 500,
  mtry  = floor(sqrt(ncol(tree_train_inner) - 1)),
  importance = TRUE
)

pred_val <- predict(rf_dto, tree_val, type = "prob")[, "YES"]
opt_rf_dto <- optimise_threshold(pred_val, y_val_tree, metric = "f1")
res_opt_rf_dto <- opt_rf_dto$all_results
cat("Validation Results")
res_opt_rf_dto[res_opt_rf_dto$threshold == opt_rf_dto$optimal_threshold,]

pred_test_dto <- predict(rf, tree_test, type = "prob")[, "YES"]
rf_class_dt_opt <- ifelse(pred_test_dto > opt_rf_dto$optimal_threshold, 1, 0)
metrics_rf_dto <- calculate_metrics(rf_class_dt_opt, y_test_tree)
metrics_rf_dto
```

## U/D Sampling

glmnet / cv.glmnet (logistic) - for family="binomial", glmnet expects a binary outcome. It accepts either numeric 0/1, or a 2-level factor (internally converted to 0/1 with the first level as 0). Turning it into numeric makes it simple and less error prone (0/1 encoding)

randomForest - in rf, the task is inferred from the type of y. if y is factor, it is classification forest (type="prob" gives class probs as mentioned earlier). if y is numeric, we will get a regression forest (continuous outputs; no type="prob").

Thus pass numeric into glm but factor into

```{r}
X_train_matrix <- as.data.frame(X_train)
y_train_fac <- droplevels(as.factor(y_train))

set.seed(123)
train_up <- upSample(x = X_train_matrix, y = y_train_fac, yname = "convert")
X_train_up <- data.matrix(subset(train_up, select = -convert))
y_train_up <- as.numeric(as.character(train_up$convert))

set.seed(123)
train_down <- downSample(x = X_train_matrix, y = y_train_fac, yname = "convert")
X_train_down <- data.matrix(subset(train_down, select = -convert))
y_train_down <- as.numeric(as.character(train_down$convert))

Train_Data_up <- Train_Data
Train_Data_up$convert <- factor(Train_Data_up$convert, levels = c("NO","YES"))

up_train <- upSample(x = subset(Train_Data_up, select = -convert),
                     y = Train_Data_up$convert, yname = "convert")

Train_Data_down <- Train_Data
Train_Data_down$convert <- factor(Train_Data_down$convert, levels = c("NO","YES"))

down_train <- upSample(x = subset(Train_Data_down, select = -convert),
                     y = Train_Data_down$convert, yname = "convert")
# prop table is just table but in proportions
# prop.table(table(y_train))
# prop.table(table(y_train_up))
# prop.table(table(y_train_down))

X_train_inner_matrix <- as.data.frame(X_train_inner)
y_train_inner_fac <- droplevels(as.factor(y_train_inner))

set.seed(123)
train_up_inner <- upSample(x = X_train_inner_matrix, y = y_train_inner_fac, yname = "convert")
X_train_inner_up <- data.matrix(subset(train_up_inner, select = -convert))
y_train_inner_up <- as.numeric(as.character(train_up_inner$convert))

set.seed(123)
train_down_inner <- downSample(x = X_train_inner_matrix, y = y_train_inner_fac, yname = "convert")
X_train_inner_down <- data.matrix(subset(train_down_inner, select = -convert))
y_train_inner_down <- as.numeric(as.character(train_down_inner$convert))

Train_Data_inner_up <- Train_Data_inner
Train_Data_inner_up$convert <- factor(Train_Data_inner_up$convert, levels = c("NO","YES"))

up_train_inner <- upSample(x = subset(Train_Data_inner_up, select = -convert),
                     y = Train_Data_inner_up$convert, yname = "convert")

Train_Data_inner_down <- Train_Data_inner
Train_Data_inner_down$convert <- factor(Train_Data_inner_down$convert, levels = c("NO","YES"))

down_train_inner <- upSample(x = subset(Train_Data_inner_down, select = -convert),
                     y = Train_Data_inner_down$convert, yname = "convert")

```


```{r}
# set seed is a random number SEQUENCE setter - use it again before each random act to make sure each random act is reproducible individually
set.seed(111)
glm_up <- glm(convert ~ ., data = up_train, family = binomial)
cv_lasso_up  <- cv.glmnet(X_train_up, y_train_up, family = "binomial", alpha = 1)
cv_ridge_up  <- cv.glmnet(X_train_up, y_train_up, family = "binomial", alpha = 0)
cv_enet_up   <- cv.glmnet(X_train_up, y_train_up, family = "binomial", alpha = 0.5)

set.seed(111)
glm_down <- glm(convert ~ ., data = down_train, family = binomial)
cv_lasso_down <- cv.glmnet(X_train_down, y_train_down, family = "binomial", alpha = 1)
cv_ridge_down <- cv.glmnet(X_train_down, y_train_down, family = "binomial", alpha = 0)
cv_enet_down <- cv.glmnet(X_train_down, y_train_down, family = "binomial", alpha = 0.5)
```

```{r}
pred_glm_up <- predict(glm_up, newdata = Test_Data, type = "response")
pred_lasso_up <- predict(cv_lasso_up, newx = X_test, type = "response", s = "lambda.min")
pred_ridge_up <- predict(cv_ridge_up, newx = X_test, type = "response", s = "lambda.min")
pred_enet_up  <- predict(cv_enet_up,  newx = X_test, type = "response", s = "lambda.min")

glm_class_up <- ifelse(pred_glm_up > 0.5, 1, 0)
lasso_class_up <- ifelse(pred_lasso_up > 0.5, 1, 0)
ridge_class_up <- ifelse(pred_ridge_up > 0.5, 1, 0)
enet_class_up  <- ifelse(pred_enet_up  > 0.5, 1, 0)

metrics_glm_up <- calculate_metrics(glm_class_up, y_test_glm)
metrics_lasso_up <- calculate_metrics(lasso_class_up, y_test)
metrics_ridge_up <- calculate_metrics(ridge_class_up, y_test)
metrics_enet_up  <- calculate_metrics(enet_class_up,  y_test)

comparison_table <- data.frame(
  GLM = metrics_glm_up,
  Lasso = metrics_lasso_up,
  Ridge = metrics_ridge_up,
  ElasticNet = metrics_enet_up
)

print(round(comparison_table, 4))
```

```{r}
pred_glm_down <- predict(glm_down, newdata = Test_Data, type = "response")
pred_lasso_down <- predict(cv_lasso_down, newx = X_test, type = "response",
                           s = "lambda.min")
pred_ridge_down <- predict(cv_ridge_down, newx = X_test, type = "response",
                           s = "lambda.min")
pred_enet_down <- predict(cv_enet_down, newx = X_test, type = "response",
                          s = "lambda.min")

glm_class_down <- ifelse(pred_glm_down > 0.5, 1, 0)
lasso_class_down <- ifelse(pred_lasso_down > 0.5, 1, 0)
ridge_class_down <- ifelse(pred_ridge_down > 0.5, 1, 0)
enet_class_down <- ifelse(pred_enet_down  > 0.5, 1, 0)

metrics_glm_down <- calculate_metrics(glm_class_down, y_test_glm)
metrics_lasso_down <- calculate_metrics(lasso_class_down, y_test)
metrics_ridge_down <- calculate_metrics(ridge_class_down, y_test)
metrics_enet_down <- calculate_metrics(enet_class_down,  y_test)

comparison_table <- data.frame(
  GLM = metrics_glm_down,
  Lasso = metrics_lasso_down,
  Ridge = metrics_ridge_down,
  ElasticNet = metrics_enet_down
)
print(round(comparison_table, 4))
```

## U/D Samp+DTO

```{r}
# attempt for u/d sampling with earlier optimal decision threshold
# up:
glm_class_up <- ifelse(pred_glm_up > opt_glm_dt_opt$optimal_threshold, 1, 0)
lasso_class_up <- ifelse(pred_lasso_up > opt_lasso_dt_opt$optimal_threshold, 1, 0)
ridge_class_up <- ifelse(pred_ridge_up > opt_ridge_dt_opt$optimal_threshold, 1, 0)
enet_class_up <- ifelse(pred_enet_up > opt_enet_dt_opt$optimal_threshold, 1, 0)

metrics_glm_up <- calculate_metrics(glm_class_up, y_test_glm)
metrics_lasso_up <- calculate_metrics(lasso_class_up, y_test)
metrics_ridge_up <- calculate_metrics(ridge_class_up, y_test)
metrics_enet_up <- calculate_metrics(enet_class_up, y_test)

comparison_table <- data.frame(
  GLM = metrics_glm_up,
  Lasso = metrics_lasso_up,
  Ridge = metrics_ridge_up,
  ElasticNet = metrics_enet_up
)

print(round(comparison_table, 4))

glm_class_down <- ifelse(pred_glm_down > opt_glm$optimal_threshold, 1, 0)
lasso_class_down <- ifelse(pred_lasso_down > opt_lasso$optimal_threshold, 1, 0)
ridge_class_down <- ifelse(pred_ridge_down > opt_ridge$optimal_threshold, 1, 0)
enet_class_down <- ifelse(pred_enet_down > opt_enet$optimal_threshold, 1, 0)

metrics_glm_down <- calculate_metrics(glm_class_down, y_test_glm)
metrics_lasso_down <- calculate_metrics(lasso_class_down, y_test)
metrics_ridge_down <- calculate_metrics(ridge_class_down, y_test)
metrics_enet_down <- calculate_metrics(enet_class_down, y_test)

comparison_table <- data.frame(
  GLM = metrics_glm_down,
  Lasso = metrics_lasso_down,
  Ridge = metrics_ridge_down,
  ElasticNet = metrics_enet_down
)
print(round(comparison_table, 4))
```

```{r}
# Actual optimisation:
set.seed(111)
glm_up_inner <- glm(convert ~ ., data = up_train_inner, family = binomial)
cv_lasso_up_inner <- cv.glmnet(X_train_inner_up, y_train_inner_up,
                          family = "binomial", alpha = 1)
cv_ridge_up_inner <- cv.glmnet(X_train_inner_up, y_train_inner_up,
                          family = "binomial", alpha = 0)
cv_enet_up_inner <- cv.glmnet(X_train_inner_up, y_train_inner_up,
                          family = "binomial", alpha = 0.5)

set.seed(111)
glm_down_inner <- glm(convert ~ ., data = down_train_inner, family = binomial)
cv_lasso_down_inner <- cv.glmnet(X_train_inner_down, y_train_inner_down,
                                 family = "binomial", alpha = 1)
cv_ridge_down_inner <- cv.glmnet(X_train_inner_down, y_train_inner_down,
                                 family = "binomial", alpha = 0)
cv_enet_down_inner <- cv.glmnet(X_train_inner_down, y_train_inner_down,
                                family = "binomial", alpha = 0.5)

```

```{r}
pred_glm_up_inner <- predict(glm_up_inner, newdata = Validation_Data, type = "response")
pred_lasso_up_inner <- predict(cv_lasso_up_inner, newx = X_val, type = "response", s = "lambda.min")
pred_ridge_up_inner <- predict(cv_ridge_up_inner, newx = X_val, type = "response", s = "lambda.min")
pred_enet_up_inner <- predict(cv_enet_up_inner,  newx = X_val, type = "response", s = "lambda.min")

pred_glm_down_inner <- predict(glm_down_inner, newdata = Validation_Data,
                               type = "response")
pred_lasso_down_inner <- predict(cv_lasso_down_inner, newx = X_val, type = "response", s = "lambda.min")
pred_ridge_down_inner <- predict(cv_ridge_down_inner, newx = X_val, type = "response", s = "lambda.min")
pred_enet_down_inner <- predict(cv_enet_down_inner,  newx = X_val, type = "response", s = "lambda.min")
```

```{r}
# up
opt_glm_up_inner <- optimise_threshold(pred_glm_up_inner,
                                       y_validation_glm, metric = "f1")
opt_lasso_up_inner <- optimise_threshold(pred_lasso_up_inner, y_val, metric = "f1")
opt_ridge_up_inner <- optimise_threshold(pred_ridge_up_inner, y_val, metric = "f1")
opt_enet_up_inner <- optimise_threshold(pred_enet_up_inner, y_val, metric = "f1")

res_glm_up_inner <- opt_glm_up_inner$all_results
res_lasso_up_inner <- opt_lasso_up_inner$all_results
res_ridge_up_inner <- opt_ridge_up_inner$all_results
res_enet_up_inner <- opt_enet_up_inner$all_results

res_glm_up_inner[res_glm_up_inner$threshold == opt_glm_up_inner$optimal_threshold,]
res_lasso_up_inner[res_lasso_up_inner$threshold == opt_lasso_up_inner$optimal_threshold,]
res_ridge_up_inner[res_ridge_up_inner$threshold == opt_ridge_up_inner$optimal_threshold,]
res_enet_up_inner[res_enet_up_inner$threshold == opt_enet_up_inner$optimal_threshold,]

# down
opt_glm_down_inner <- optimise_threshold(pred_glm_down_inner,
                                         y_validation_glm, metric = "f1")
opt_lasso_down_inner <- optimise_threshold(pred_lasso_down_inner, y_val, metric = "f1")
opt_ridge_down_inner <- optimise_threshold(pred_ridge_down_inner, y_val, metric = "f1")
opt_enet_down_inner <- optimise_threshold(pred_enet_down_inner, y_val, metric = "f1")

res_glm_down_inner <- opt_glm_down_inner$all_results
res_lasso_down_inner <- opt_lasso_down_inner$all_results
res_ridge_down_inner <- opt_ridge_down_inner$all_results
res_enet_down_inner <- opt_enet_down_inner$all_results
res_glm_down_inner[res_glm_down_inner$threshold == opt_glm_down_inner$optimal_threshold,]
res_lasso_down_inner[res_lasso_down_inner$threshold ==
                       opt_lasso_down_inner$optimal_threshold,]
res_ridge_down_inner[res_ridge_down_inner$threshold ==
                       opt_ridge_down_inner$optimal_threshold,]
res_enet_down_inner[res_enet_down_inner$threshold ==
                      opt_enet_down_inner$optimal_threshold,]
```

```{r}
# test set
# up
pred_glm_up_inner <- predict(glm_up_inner, newdata = Test_Data, type = "response")
pred_lasso_up_inner <- predict(cv_lasso_up_inner, newx = X_test, type = "response",
                      s = "lambda.min")
pred_ridge_up_inner <- predict(cv_ridge_up_inner, newx = X_test, type = "response",
                      s = "lambda.min")
pred_enet_up_inner <- predict(cv_enet_up_inner,  newx = X_test, type = "response",
                     s = "lambda.min")

glm_class_up_inner <- ifelse(pred_glm_up_inner > 
                               opt_glm_up_inner$optimal_threshold, 1, 0)
lasso_class_up_inner <- ifelse(pred_lasso_up_inner >
                                 opt_lasso_up_inner$optimal_threshold, 1, 0)
ridge_class_up_inner <- ifelse(pred_ridge_up_inner >
                                 opt_ridge_up_inner$optimal_threshold, 1, 0)
enet_class_up_inner  <- ifelse(pred_enet_up_inner  >
                                 opt_enet_up_inner$optimal_threshold, 1, 0)

metrics_glm_up_inner <- calculate_metrics(glm_class_up_inner, y_test_glm)
metrics_lasso_up_inner <- calculate_metrics(lasso_class_up_inner, y_test)
metrics_ridge_up_inner <- calculate_metrics(ridge_class_up_inner, y_test)
metrics_enet_up_inner <- calculate_metrics(enet_class_up_inner, y_test)

comparison_table_up_inner <- data.frame(
  GLM = metrics_glm_up_inner,
  Lasso = metrics_lasso_up_inner,
  Ridge = metrics_ridge_up_inner,
  ElasticNet = metrics_enet_up_inner
)
comparison_table_up_inner

# down

pred_glm_down_inner <- predict(glm_down_inner, newdata = Test_Data, type = "response")
pred_lasso_down_inner <- predict(cv_lasso_down_inner, newx = X_test, type = "response",
                      s = "lambda.min")
pred_ridge_down_inner <- predict(cv_ridge_down_inner, newx = X_test, type = "response",
                      s = "lambda.min")
pred_enet_down_inner <- predict(cv_enet_down_inner,  newx = X_test, type = "response",
                     s = "lambda.min")

glm_class_down_inner <- ifelse(pred_glm_down_inner >
                                 opt_glm_down_inner$optimal_threshold, 1, 0)
lasso_class_down_inner <- ifelse(pred_lasso_down_inner >
                                   opt_lasso_down_inner$optimal_threshold, 1, 0)
ridge_class_down_inner <- ifelse(pred_ridge_down_inner >
                                   opt_ridge_down_inner$optimal_threshold, 1, 0)
enet_class_down_inner  <- ifelse(pred_enet_down_inner >
                                   opt_enet_down_inner$optimal_threshold, 1, 0)

glm_class_up_inner <- ifelse(pred_glm_up_inner > 0.5, 1, 0)
lasso_class_up_inner <- ifelse(pred_lasso_up_inner > 0.5, 1, 0)
ridge_class_up_inner <- ifelse(pred_ridge_up_inner > 0.5, 1, 0)
enet_class_up_inner  <- ifelse(pred_enet_up_inner > 0.5, 1, 0)

metrics_glm_down_inner <- calculate_metrics(glm_class_down_inner, y_test_glm)
metrics_lasso_down_inner <- calculate_metrics(lasso_class_down_inner, y_test)
metrics_ridge_down_inner <- calculate_metrics(ridge_class_down_inner, y_test)
metrics_enet_down_inner <- calculate_metrics(enet_class_down_inner, y_test)

comparison_table_down_inner <- data.frame(
  GLM = metrics_glm_down_inner,
  Lasso = metrics_lasso_down_inner,
  Ridge = metrics_ridge_down_inner,
  ElasticNet = metrics_enet_down_inner
)
comparison_table_down_inner

```

seems like overall performace is better without up or down sampling maybe? need to think about what metrics we really want to be using.

# Next Steps - tuning, feature selection, weights, analysis of error and residuals

To do:
-   tune hyper parameters
-   do better feature selection —\> hybrid/forward/backward subset feature selection
-   try the weights 4 to 1 thing for lasso/enet after optimising decision boundary
-   check if under or overfitting by comparing pure train and test error

```{r}
cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1)
train_preds <- predict(cv_lasso, newx = X_train, s = "lambda.min", type = "response")
test_preds <- predict(cv_lasso, newx = X_test, s = "lambda.min", type = "response")

opt_threshold <- opt_lasso$optimal_threshold
train_class <- ifelse(train_preds > opt_threshold, 1, 0)
test_class <- ifelse(test_preds > opt_threshold, 1, 0)

train_error <- mean(train_class != y_train)
test_error <- mean(test_class != y_test)

cat("Training error:", round(train_error, 3), "\n")
cat("Test error:", round(test_error, 3))
```

# underfitting? maybe a discuss for next week.

```{r}
library(MASS)
step_model <- stepAIC(glm_basic, direction = "both", trace = FALSE)
summary(step_model)
formula(step_model)
```

-   stratified samplied didn't do much, the original sampling was already relatively well split between train and test for the target variable.

-   tested model with more significant predictors, didn't change model too much

-   tested using weights in cv.glmnet, worse overall than decision threshold optimisation

To Do:

-   loads of testing with different models, with different params, before and after box cox or z scaling

-   loads of testing to do with hyperparams
