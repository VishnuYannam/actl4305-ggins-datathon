---
title: "EDA_and_modelling_5oct"
author: "Michael Wang"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Pre-processing Before EDA

Import Libraries

```{r}
library(tidyverse)
```

Read in the data

```{r}
df <- read_csv('standardised_freely_quote_data_v2.csv')
crime_indices <- read_csv('cleaned_crime_indices.csv')
```

trip main df columns a bit

```{r}
df_trim <- df %>% 
  select(-destinations, -traveller_ages, -quote_create_time, -matches("^boost_.*_date$"))
```

Deal with remaining NA's in the data

```{r}
df_clean <- df_trim %>%
  mutate(
    across(starts_with("boost_"), ~ if_else(is.na(.x), 0, .x)),
    quote_hour = if_else(quote_hour > 23, 24, quote_hour)
    )

any(is.na(df_clean))
```

Glimpse tells us we should turn discount into a number, and then turn the remaining character classes into factors

```{r}
glimpse(df_clean)
```

```{r}
df_discount <- df_clean %>%
  mutate(
    discount = as.numeric(str_extract(discount, "^[0-9]*"))
  )

# gets a names vector, then applies the logical mask to extract only the columns.
character_colnames <- names(df_discount)[sapply(df_discount, is.character)]

# all_of interprets a character vector as column names
df_factors <- df_discount %>%
  mutate(
    across(all_of(character_colnames), ~ as.factor(.x))
  )
```

```{r}
glimpse(df_factors)
```

Attach crime scores

```{r}
df_crime_scores <- df_factors %>%
  mutate(quote_id = row_number()) %>%
  left_join(crime_indices, by = "quote_id") %>%
  select(-quote_id)

df_crime_scores <- df_crime_scores %>%
  rowwise() %>%
  mutate(
    median_crime_index = median(as.numeric(unlist(strsplit(crime_index, ", "))))
  ) %>%
  ungroup %>%
  select(-crime_index)
```

```{r}
plot(df_crime_scores$quote_hour)
```

```{r}
df_clean_quote_hour <- df_crime_scores %>%
  mutate(quote_hour = if_else(quote_hour > 23, 24, quote_hour))
```

```{r}
table(df_clean_quote_hour$extra_cancellation)
```

```{r}
df_extra_cancellation <- df_clean_quote_hour %>%
  mutate(
    extra_cancel_tier = case_when(
      extra_cancellation == 0 ~ 0,
      extra_cancellation <= 5000 ~ 1,
      extra_cancellation > 5000 ~ 2,
      TRUE ~ 1
    )
  ) %>%
    select(-extra_cancellation)

df_final <- df_extra_cancellation %>%
  select(-quote_time)

```

Backup before eda

```{r}
df_eda <- df_final
```

```{r}
df_write <- df_eda

write_csv(
  df_eda,"~/Desktop/UNSW/Year 4/Term 3/ACTL4305/Assignment/final_eda_modelling_data.csv"
)
```

==================================

# EDA

```{r}
# install.packages("DataExplorer")
# install.packages("skimr")
library(DataExplorer)
library(skimr)
```

```{r}
plot_intro(df_eda)
```

```{r}
skim(df_eda)
```

Variables to process outliers first:

-   quote price, lead length, trip length,

Variables to transform first before checking for outliers:

```{r}
skim_df <- skim(df_eda)
```

Overall data frame:

1.  median start date: 2024 12 27
2.  median end date: 2025 01 18
3.  mean quote price 361, median 201 â€”\> right skewed with some large outliers (super long trips probably)
4.  mean quote hour is 14.6, median is 14 very similar. Approx 2pm is best for some reason
5.  mean trip length is 27 days, median is 15 days. Confirms number 3
6.  mean lead length is 60 days, median 25
7.  mean and median 1.8/2 travelers
8.  mean/median max age 50/51
9.  mean/median min age 42/39 so high...?
10. mean age range 8, median 0
11. mean of mean and median age 26, median is 42/44
12. mean/median 1.22/1 generation
13. mean/median 0.5/0 boosts
14. 0.026 proportion to africa, 0.188 to europe, 0.221 to oceania, 0.028 to multi region, 0.065 domestic cruise, 0.008 international cruise, 0.121 north america, 0.006 central america, 0.017 south america, 0.029 middle east, 0.003 central asia, 0.296 south east asia, 0.034 south asia, 0.147 east asia, 0.002 antarctica (doesn't sum to one since people go to multiple places. Sums to approx 1.191)
15. 0.1 have kids under 12, 0.069 has teens over 13, 0.75 has adults O 18, 0.3 have seniors O 60, 0.15 is family.
16. top quote times are around 12pm, 4pm, and 1pm
17. imbalanced data â€”\> 60999 NO, 9001 YES
18. 0.065 have snowsports, 0.009 adventure activities, 0.105 cruise cover, 0.001 medical, 0.033 gadget, 0.025 motorcycle, 0.016 rental vehicle excess, 0.008 specified items.
19. avg 1.6 adults
20. mean of median crime index of all visited destinations is 42.44, median 46.1 - slightly right skewed
21. not sure if boost lengths will tell anything because they represent different things.
    1.  boost 1: mean length 9 days
    2.  boost 2: 1.619 days
    3.  boost 3: 0.366 days
    4.  boost 4: 0.134 days
    5.  boost 5: 0.056 days
    6.  boost 6: 0.027 days
    7.  boost 7: 0.015 days
    8.  boost 8: 0.013 days
22. For each boost length, max value is VERY LARGE. 547, 545, 463, 365, 347, 192, 192, 192

```{r}
plot_bar(df_eda)
```

1.  large amount of quotes coming from O60 couples, single U60, single O60, single U30, couple U60, and then parents plus kids. Remaining probably is \< 10000 total quotes.
2.  most is 15 or 0 discount, makes sense cause thats the auto discount for couple or single as above.

```{r}
plot_histogram(df_eda)
```

```{r}
plot_density(df_eda)
```

Mean age bimodal at 25 and 60 ish. Num travellers spikes at 1 and 2. Quote hour max at around 14-15. Boost num spikes at 0 and 1. Crime index spikes at approx 48.

Split convert and no convert and visualise

```{r}
df_eda_convert <- df_eda %>%
  filter(convert == "YES")

df_eda_nonconvert <- df_eda %>%
  filter(convert == "NO")
```

```{r}
skim(df_eda_convert)
```

```{r}
skim_df_convert <- skim(df_eda_convert)
```

1.  has child 0.117, has teen 0.082, has adult 0.878, has senior 0.1667, is family 0.179
2.  quote price 193 mean 132 median
3.  extra cancellation 271 mean 0 median
4.  quote hour 14.7 mean 15 median.
5.  trip length 20 days mean 13 days median
6.  lead length 33.6 mean 10 median
7.  num travellers 1.8/2 mean/median
8.  44.8/43 max age mean/median, 35.7/30 min mean/median
9.  age range mean 9, median 0
10. mean/median age, mean is 40 median is 34.
11. 1.2/1 generations mean/median
12. 0.66/0 boosts mean/median
13. 0.019 Africa, 0.155 europe, 0.188 oceania, 0.020 multi region, 0.072 domestic cruise, 0.010 international cruise, 0.089 north america, 0.003 central america, 0.013 south america, 0.018 middle east, 0.001 central asia, 0.355 south east asia, 0.026 south asia, 0.202 east asia, 0.001 antarctica
14. 0.152 snowsports, 0.020 adventure, 0.100 cruise cover, 0.001 medical, 0.084 gadget, 0.069 motorcycle, 0.023 rental vehicle excess, 0.010 specified items
15. 1.6 mean adults, 1 median
16. mean median crime index over all locations is 41, median is 46. Max median crime score is 81.
17. Boost lengths: 5.8, 2.7, 0.7, 0.27, 0.11, 0.03, 0.016, 0.015 mean. Max: 539, 545, 364, 364, 347, 30, 38, 38.

```{r}
skim(df_eda_nonconvert)
```

```{r}
skim_df_nonconvert <- skim(df_eda_nonconvert)
```

1.  0.1 has child u12, 0.067 has teen o13, 0.73 have adults 018, 0.32 have seniors o65, 0.15 are families
2.  386 mean quote price, 215 median quote price
3.  1755 mean extra cancellation, 0 median
4.  quote hour mean 14.5, median 14
5.  28 day mean trip length, 16 median
6.  1.83/2 mean/median travelers
7.  51/53 max age mean/median
8.  43/41 min age mean/median
9.  8/0 age range mean/median
10. 47/45, 47/46 mean/median of mean age and median age
11. 1.2/1 mean/median num generations
12. 0.55/0 mean/median boost number
13. 0.027 africa, 0.194 europe, 0.226 oceania, 0.029 multi region, 0.064 domestic cruise, 0.008 int cruise, 0.126 north america, 0.006 central america, 0.018 south america, 0.024 middle east, 0.001 central asia, 0.288 south east asia, 0.035 south asia, 0.139 east asia, 0.002 antarctica
14. 0.052 snow sports, 0.007 adventure, 0.107 cruise cover, 0.0007 medical, 0.025 gadget cover, 0.018 motorcycle, 0.015 rental vehicle excess, 0.008 specific items
15. 1.6/2 mean/median num adults
16. boost lengths:
    1.  9.67, 1.46, 0.312, 0.114, 0.048, 0.027, 0.015, 0.013,
17. crime score mean 42.65, median 46.1

```{r}
plot_bar(df_eda_convert)
```

For converted quotes:

-   mostly single U60 and single U30, then couple U60, couple O60, then single O60, parnets plus kids, couples U30, parents plus teens. everything else is less than 500 out of 9001

```{r}
plot_bar(df_eda_nonconvert)
```

For non-converted:

-   mostly couples O60, single O60, single U60, single U30, couple U60, parents plus kids, couple U30. Everything else under 4000 out of 60999

```{r}
plot_histogram(df_eda_convert)
```

For converted quotes:

-   mean age is much more centered around 25 as opposed to bimodal in overall data. boost number is larger in general

-   Everything else very similar.

```{r}
plot_histogram(df_eda_nonconvert)
```

For non-convert:

-   ages are split better between the two peaks 25 and 60, less boosts overall, longer lead lengths, generally older population.

-   everything else very similar.

```{r}
plot_density(df_eda_convert)
```

```{r}
plot_density(df_eda_nonconvert)
```

EDA DONE:

-   need to deal with outliers for a few variables to remove possible errors. Winsorize and cap at 99th percentile

    -   quote price, trip length, lead length, boost lengths.

-   box cox for positive non-normal continuous columns, then z score scale them:

    -   all above + age vars +

-   add binary flag if 0's dominate. Otherwise z score scale â€“\> discrete, count based variables with low ranges

# WINSORISATION

only 200 quotes over 5000, and even less super large ones. 173 trips over a year long. Winsorize/cap at 99th percentile.

```{r}
df_large_quote_price <- df_eda[df_eda$quote_price > 5000,]
df_large_lengths <- df_eda[df_eda$trip_length > 365,]
```

```{r}
df_capped <- df_eda
```

```{r}
cap_trip_len <- quantile(df_capped$trip_length, 0.99)
df_capped$trip_length_capped <- pmin(df_capped$trip_length, cap_trip_len)

cap_price <- quantile(df_capped$quote_price, 0.99)
df_capped$quote_price_capped <- pmin(df_capped$quote_price, cap_price)

cap_lead_len <- quantile(df_capped$lead_length, 0.99)
df_capped$lead_length_capped <- pmin(df_capped$lead_length, cap_lead_len)

boost_cols <- paste0("boost_", 1:8, "_length")
for (col in boost_cols) {
  cap_val <- quantile(df_capped[[col]], 0.99)
  df_capped[[col]] <- pmin(df_capped[[col]], cap_val)
}

cap_age_range <- quantile(df_capped$age_range, 0.99)
df_capped$age_range_capped <- pmin(df_capped$age_range, cap_age_range)

df_capped <- df_capped %>%
  select(-trip_length, -quote_price, -lead_length, -age_range)
```

# Correlation Analysis

```{r}
df_correlation <- df_capped %>%
  select(-median_age, -min_age, -max_age)

cor(df_correlation[ , sapply(df_correlation, is.numeric)])
```

Main ones over 0.2 or less than -0.2:

discount and num travellers 0.65

boost num and boost 1, 2, 3 (2 \> 3 \> 1) 0.55, 0.5, 0.45

boost num and snow sports 0.33

domestic cruise and oceania 0.49

oceania and south east asia -0.33

boost num and cruise cover 0.41

snow sports and east asia 0.28

boost num and gadget 0.35

crime index and east asia -0.69, and oceania 0.29, africa 0.23

extra cancel teir with discount 0.49, with discount 0.2

trip length capped and europe 0.21

quote price capped and mean age?? 0.33 and north america is 0.32,

lead length capped and mean age 0.35, and europe is 0.21

age range capped and discount 0.41, num travellers 0.8, mean age -0.3, generations 0.79

mean age and domestic cruise 0.2

north america and south east asia -0.22

```{r}
df_correlation <- df_correlation %>%
  select(-generations, -age_range_capped, -boost_4_length, -boost_5_length, -boost_6_length, -boost_7_length, -boost_8_length)
```

```{r}
df_transformations <- df_correlation
```

# Box-Cox + z Score Transformations

```{r}
library(caret)
```

Box cox all the age related ones, the length related ones and the price.

```{r}
box_cox_cols <- c(
  "mean_age",
  "trip_length_capped",
  "quote_price_capped",
  "lead_length_capped",
  paste0("boost_", 1:3, "_length")
)

small <- 1e-6
for (col in box_cox_cols) {
  zero_flag <- min(df_transformations[[col]])
  if (zero_flag <= 0) {
    df_transformations[[col]] <- df_transformations[[col]] + small
  }
}

X <- as.data.frame(df_transformations[, box_cox_cols])

preprocess_boxcox <- preProcess(X, method = "BoxCox")
X_boxcox  <- predict(preprocess_boxcox, X)

df_transformations[paste0(names(X_boxcox), "_boxcox")] <- X_boxcox

boxcox_cols <- paste0(box_cox_cols, "_boxcox")
X_boxcox <- as.data.frame(df_transformations[, boxcox_cols])
preprocess_z <- preProcess(X_boxcox, method = c("center", "scale"))
X_boxcox_z <- predict(preprocess_z, X_boxcox)
df_transformations[paste0(box_cox_cols, "_boxcox_z")] <- X_boxcox_z

df_transformations <- df_transformations %>%
  select(-ends_with("_boxcox"), -all_of(box_cox_cols))
```

```{r}
df_transformations_1 <- df_transformations
```

\
z score transform everything above + all the discrete ones (discount, boost num, num travellers and adults, generations) + crime index

```{r}
remaining_z_cols <- c(
  "median_crime_index",
  "num_adults",
  "boost_num",
  "num_travellers",
  "discount"
)

small <- 1e-6
for (col in remaining_z_cols) {
  zero_flag <- min(df_transformations_1[[col]])
  if (zero_flag <= 0) {
    df_transformations_1[[col]] <- df_transformations_1[[col]] + small
  }
}

X_z_df <- as.data.frame(df_transformations_1[, remaining_z_cols])
preprocess_z <- preProcess(X_z_df, method = c("center", "scale"))
X_z <- predict(preprocess_z, X_z_df)
df_transformations_1[paste0(remaining_z_cols, "_z")] <- X_z

df_transformations_1 <- df_transformations_1 %>%
  select(-all_of(remaining_z_cols))
```

```{r}
summary(df_transformations_1)
```

```{r}
glimpse(df_transformations_1)
```

### Vishnu Insights

go into it deeper and find specifics

visualise quote price vs conversion and non-conversion

relationships between columns and target variables

## **Conversion Funnel by Platform**

```{r}
library(dplyr)
library(ggplot2)
library(scales)

# Prepare funnel data
funnel_df <- df %>%
  mutate(any_boost = if_any(
    c(snowsports, adventure_activities, cruise_cover,
      medical_conditions, gadget_cover, motorcycle_cover,
      rental_vehicle_excess, specified_items, extra_cancellation),
    ~ . > 0 | . == TRUE
  )) %>%
  mutate(stage = case_when(
    convert == "YES" ~ "Converted",
    any_boost        ~ "With Boost",
    TRUE             ~ "All Quotes"
  )) %>%
  group_by(platform, stage) %>%
  summarise(n = n(), .groups = "drop") %>%
  mutate(stage = factor(stage, levels = c("All Quotes", "With Boost", "Converted")))

# Clean and aesthetic funnel plot
ggplot(funnel_df, aes(x = stage, y = n, fill = platform)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.6, color = "white", linewidth = 0.3) +
  scale_y_continuous(labels = comma_format(), expand = expansion(mult = c(0, 0.05))) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "ðŸª„ Customer Funnel by Platform",
    subtitle = "Progression of quotes through key stages across platforms",
    x = NULL,
    y = "Number of Quotes",
    fill = "Platform",
    caption = "Source: internal dataset"
  ) +
  theme_minimal(base_family = "Helvetica", base_size = 13) +
  theme(
    plot.title = element_text(size = 15, face = "bold", color = "#333333"),
    plot.subtitle = element_text(size = 11, color = "#666666"),
    axis.text = element_text(size = 10, color = "#333333"),
    axis.title = element_text(size = 11),
    legend.position = "top",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(color = "grey90"),
    plot.caption = element_text(size = 8, color = "grey50", hjust = 1)
  )
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(forcats)
library(lubridate)
library(scales)
theme_set(theme_minimal(base_size = 13))
```

Conversion Rate vs Quote Price

```{r}
library(ggplot2)
library(scales)
library(dplyr)

df <- df %>%
  mutate(price_decile = ntile(quote_price, 10))

conv_price <- df %>%
  group_by(price_decile) %>%
  summarise(
    conv_rate = mean(convert == "YES"),
    avg_price = mean(quote_price),
    .groups = "drop"
  )

ggplot(conv_price, aes(x = avg_price, y = conv_rate)) +
  geom_line(size = 1.2, color = "#0073C2FF", alpha = 0.9) +
  geom_point(size = 3, color = "white", fill = "#0073C2FF", shape = 21, stroke = 0.6) +
  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.05))) +
  scale_x_continuous(labels = dollar_format(accuracy = 1), expand = expansion(mult = c(0.02, 0.05))) +
  labs(
    title = "ðŸ’¸ Conversion Rate vs Quote Price",
    subtitle = "Conversion declines as average quote price increases (binned by decile)",
    x = "Average Quote Price (Decile Mean)",
    y = "Conversion Rate"
  ) +
  theme_minimal(base_family = "Helvetica", base_size = 13) +
  theme(
    plot.title = element_text(size = 15, face = "bold", color = "#333333"),
    plot.subtitle = element_text(size = 11, color = "#666666"),
    axis.text = element_text(size = 10, color = "#333333"),
    axis.title = element_text(size = 11),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_line(color = "grey90"),
    panel.grid.major.y = element_line(color = "grey90")
  )
```

Heatmap of Conversion by Trip Length

```{r}
heat_df <- df %>%
  mutate(lead_bin = cut(lead_length,
                        breaks = c(0,7,14,30,60,120,Inf),
                        labels = c("0â€“7","8â€“14","15â€“30","31â€“60","61â€“120","120+")),
         trip_bin = cut(trip_length,
                        breaks = c(0,7,14,21,30,60,Inf),
                        labels = c("0â€“7","8â€“14","15â€“21","22â€“30","31â€“60","60+"))) %>%
  group_by(lead_bin, trip_bin) %>%
  summarise(conv_rate = mean(convert == "YES"), .groups = "drop")

ggplot(heat_df, aes(x = lead_bin, y = trip_bin, fill = conv_rate)) +
  geom_tile(colour = "white") +
  scale_fill_gradient(low = "lightyellow", high = "steelblue", labels = percent) +
  labs(title = "Conversion by Lead Time and Trip Length",
       x = "Lead Time (days)", y = "Trip Length (days)",
       fill = "Conv. Rate") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Top 10 Destinations by Conversion Rate

```{r}
top_dests <- df %>%
  group_by(destinations) %>%
  summarise(conv_rate = mean(convert == "YES"),
            n = n(), .groups = "drop") %>%
  filter(n > 200) %>%                  # exclude tiny groups
  top_n(10, conv_rate) %>%
  mutate(destinations = fct_reorder(destinations, conv_rate))

ggplot(top_dests, aes(x = destinations, y = conv_rate)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  scale_y_continuous(labels = percent) +
  labs(title = "Top 10 Destinations by Conversion Rate",
       x = NULL, y = "Conversion Rate")
```

Boost Type vs Conversion Uplift

```{r}
boost_cols <- c("snowsports","adventure_activities","cruise_cover",
                "medical_conditions","gadget_cover","motorcycle_cover",
                "rental_vehicle_excess","specified_items","extra_cancellation")

boost_df <- df %>%
  pivot_longer(cols = all_of(boost_cols),
               names_to = "boost_type", values_to = "selected") %>%
  group_by(boost_type) %>%
  summarise(conv_rate = mean(convert == "YES" & selected == 1),
            base_rate = mean(convert == "YES"),
            .groups = "drop") %>%
  mutate(uplift = conv_rate - base_rate,
         boost_type = fct_reorder(boost_type, uplift))

ggplot(boost_df, aes(x = boost_type, y = uplift)) +
  geom_col(fill = "seagreen") +
  coord_flip() +
  scale_y_continuous(labels = percent) +
  labs(title = "Conversion Uplift by Boost Type",
       x = NULL, y = "Uplift vs Baseline")
```

Age Group vs Conversion

```{r}
df <- df %>%
  mutate(age_group = case_when(
    mean_age < 30 ~ "<30",
    mean_age < 45 ~ "30â€“44",
    mean_age < 60 ~ "45â€“59",
    TRUE          ~ "60+"
  ))

ggplot(df, aes(x = age_group, fill = convert)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) +
  scale_fill_manual(values = c("NO"="grey70","YES"="steelblue")) +
  labs(title = "Conversion by Age Group",
       x = "Age Group", y = "Proportion of Quotes", fill = "Converted")
```

Platform and Hour-of-day-Effects

```{r}
df <- df %>%
  mutate(hour = hour(quote_create_time))

conv_hour <- df %>%
  group_by(hour, platform) %>%
  summarise(conv_rate = mean(convert == "YES"), .groups = "drop")

ggplot(conv_hour, aes(x = hour, y = conv_rate, colour = platform)) +
  geom_line(size = 1) +
  scale_y_continuous(labels = percent) +
  labs(title = "Conversion Rate by Hour of Day and Platform",
       x = "Hour of Day", y = "Conversion Rate")
```

Quote Price vs Conversion

```{r}
cap <- quantile(df$quote_price, 0.99, na.rm = TRUE)
df <- df %>% mutate(quote_price_capped = pmin(quote_price, cap))

ggplot(df, aes(x = convert, y = quote_price_capped, fill = convert)) +
  geom_boxplot(outlier.alpha = 0.3) +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(title = "Quote Price Distribution (Capped at 99th Percentile)",
       x = "Conversion Status", y = "Quote Price (capped)")
```

Price vs Conversion Density

```{r}
ggplot(df, aes(x = quote_price, fill = convert)) +
  geom_density(alpha = 0.4) +
  scale_x_log10(labels = scales::dollar_format()) +
  scale_fill_manual(values = c("NO" = "grey70", "YES" = "steelblue")) +
  labs(title = "Density of Quote Price by Conversion Status",
       x = "Quote Price (log scale)", y = "Density", fill = "Converted") +
  theme_minimal(base_size = 14)
```

Conversion by Platform

```{r}
ggplot(df, aes(x = platform, fill = convert)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent) +
  scale_fill_manual(values = c("NO" = "grey70", "YES" = "steelblue")) +
  labs(title = "Conversion Rate by Platform",
       x = "Platform", y = "Proportion of Quotes", fill = "Converted") +
  theme_minimal(base_size = 14)
```
